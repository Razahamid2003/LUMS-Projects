{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CS-331: Introduction to Artificial Intelligence - Spring 2024**\n",
    "\n",
    "# Assignment 2: Logistic, Lasso, and Ridge Regression\n",
    "\n",
    "### Deadline:  12 March 2024 11:55 PM\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. The aim of the assignment is to familiarise you with code implementation of concepts covered in class as well as learn thorough data analysis.\n",
    "\n",
    "2. All cells must be run once before submission and should $\\color{orange}{\\text{clearly display the results (graphs/plots/visualizations etc)}}$. Failure to do so will result in deduction of points.\n",
    "\n",
    "5. Use procedural programming style and comment your code properly. The grading breakdown has $\\color{orange}{\\text{five}}$ points reserved for well-commented, modular code. \n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "1. You are required to upload your solutions on LMS in the Assignment tab by the deadline. \n",
    "\n",
    "2. The zip file MUST contain your jupyter notebook file (.ipynb) and python script (.py) files.\n",
    "\n",
    "3. **EACH** file should be named as \"Name_RollNumber_PA $\\color{orange}{\\text{x}}$\" where $\\color{orange}{\\text{x = 2}}$ for this assignment. \n",
    "\n",
    "### Late Submission Policy\n",
    "\n",
    "1. You can submit upto 2 days late with a 10% deduction penalty (from your final obtained marks) for each late day.\n",
    "2. $\\color{red}{\\text{No submission}}$ will be accepted after the late days are over.\n",
    "2. $\\color{red}{\\text{No submission}}$ will be accepted via email/Slack, unless announced by the course staff beforehand. It is your responsibility to stay updated with any announcements pertaining to the assignment. \n",
    "\n",
    "### Plagiarism Policy\n",
    "All parts of this assignment are to be done $\\color{red}{\\text{INDEPENDENTLY}}$. The course stuff will refer any case of plagiarism \n",
    "from others or the internet immediately to the DC. If you are confused about what\n",
    "constitutes plagiarism, it is your responsibility to consult with the TAs\n",
    "in a timely manner.\n",
    "\n",
    "### Vivas\n",
    "\n",
    "The teaching staff reserves the right to conduct a viva for any student.  \n",
    "\n",
    "### Notes\n",
    "The required packages for this assignment are already present in the first cell.\n",
    "\n",
    "If you are running the noteook on your PC/laptop, it is STRONGLY advised that you install [conda](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html) and work in a conda environment for this and future assignments. This will go a long way in ensuring you do not get dependency conflicts and your system does not slow down (believe me conflics actually happen and have cost me my sanity).\n",
    "\n",
    "### Conda Instructions\n",
    "\n",
    "After installing [conda](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html), open a terminal/command prompt window in the assignment folder and run `conda env create -f environment.yml` command to automatically create an isolated conda environment titled `AI_env` with required packages installed. Then open/restart VSCode to select this environment as the kernel for your notebook. Your first cell should now compile smoothly! \n",
    "\n",
    "P.S: You can also check the output of the command `conda list -n AI_env` to see if each required package listed in the `environment.yml` file is present in this environment.\n",
    "\n",
    "If you are working on Google Colab, you do not need this. In case the `root_mean_squared` function throws an import error, just use the mean squared error with the argument `squared` set to `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error # set 'squared' argument to false to get RMSE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1A: Multinomial Logistic Regression\n",
    "\n",
    "### Problem Introduction\n",
    "\n",
    "This task is intended to give you a solid understanding of logistic regression.\n",
    "\n",
    "Logistic regression is primarily employed for classification problems. \n",
    "\n",
    "**Binary (or simple) logistic regression** deals with scenarios where the dependent variable has two possible outcomes e.g. yes/no, true/false etc. It models the probability of an observation belonging to one of the two categories.\n",
    "\n",
    "**Multinomial logistic regression** extends binary logistic regression to handle situations with more than two categories (yes/no/maybe) for the dependent variable. It's useful for classification problems involving multiple classes. In this part, you will be implementing a model that can handle classification in one such multiclass case.\n",
    "\n",
    "### Dataset\n",
    "The dataset for this task is provided in a csv titled `star_classification.csv`. It consists of 100,000 observations of space taken by the SDSS (Sloan Digital Sky Survey). Every observation is described by 17 feature columns and 1 class column which identifies it to be either a star, galaxy or quasar. Following is the description of each column in the dataset.\n",
    "\n",
    "**obj_ID** = Object Identifier, the unique value that identifies the object in the image catalog used by the CAS  \n",
    "\n",
    "**alpha** = Right Ascension angle (at J2000 epoch)  \n",
    "\n",
    "**delta** = Declination angle (at J2000 epoch)  \n",
    "\n",
    "**u** = Ultraviolet filter in the photometric system  \n",
    "\n",
    "**g** = Green filter in the photometric system  \n",
    "\n",
    "**r** = Red filter in the photometric system  \n",
    "\n",
    "**i** = Near Infrared filter in the photometric system  \n",
    "\n",
    "**z** = Infrared filter in the photometric system  \n",
    "\n",
    "**run_ID** = Run Number used to identify the specific scan  \n",
    "\n",
    "**rereun_ID** = Rerun Number to specify how the image was processed  \n",
    "\n",
    "**cam_col** = Camera column to identify the scanline within the run  \n",
    "\n",
    "**field_ID** = Field number to identify each field  \n",
    "\n",
    "**spec_obj_ID** = Unique ID used for optical spectroscopic objects (this means that 2 different observations with the same spec_obj_ID must share the output class)  \n",
    "\n",
    "**class** = Object class (galaxy, star, or quasar object) $\\color{green}{\\text{-> This column has the labels that your classifier will predict}}$\n",
    "\n",
    "**redshift** = Redshift value based on the increase in wavelength  \n",
    "\n",
    "**plate** = Plate ID, identifies each plate in SDSS  \n",
    "\n",
    "**MJD** = Modified Julian Date, used to indicate when a given piece of SDSS data was taken  \n",
    "\n",
    "**fiber_ID** = Fiber ID that identifies the fiber that pointed the light at the focal plane in each observation \n",
    "\n",
    "### Task\n",
    "\n",
    "Your objective will be to build a classifier that can classify the `class` as either `star`, `galaxy` or `quasar` for instances in the test set.\n",
    "\n",
    "### Note\n",
    "\n",
    "You are $\\color{red}{\\text{NOT}}$ allowed to use `scikit-learn` for any part of this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To start off, load the csv as a pandas `DataFrame`, visualize it and report its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now split the dataset into train and test sets. Choose the split ratio based on your intuition and knowledge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have loaded the data and understood its structure, you will do some preprocessing to get features and class labels/categories for use with your classifier:\n",
    "\n",
    "1. Make new DataFrames labelled `X_train` and `X_test` that have the feature columns only. Remove any features that you consider irrelevant for training the classifier. You could use trial and error to observe which set of attributes give you the best accuracy. (HINT: Check the dataset description given earlier to decide which columns might be dropped). \n",
    "\n",
    "2. Normalize both train and test feature matrices. (Caveat: make sure to normalize test set appropriately). \n",
    "\n",
    "3. Report normalized `X_train` and `X_test` shape and state what each dimension of shape represents. \n",
    "\n",
    "4. Incorporate a bias vector of appropriate shape (determine this yourself) to the normalized feature matrices.\n",
    "\n",
    "5. Report first instance of normalized train and test feature matrices before and after adding the bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Extract the class labels from both train and test dataset, and convert to NumPy array for faster preprocessing. Label these `y_train` and `y_test`. (HINT: Check dataset description to see which column to extract).\n",
    "\n",
    "7. Report `y_train` and `y_test` shape and state what each dimension of shape represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you would have seen, `y_train` and `y_test` have class labels/categories in string format right now. However, many machine learning algorithms, such as logistic regression, support only numerical input features. They are designed to perform mathematical operations on numerical data, such as addition, subtraction, and multiplication. Therefore, we will first encode these string labels as integers, and then one-hot encode them.\n",
    "\n",
    "7. Create a mapping that converts the current class labels to integers e.g. Class1: 0, Class2: 1 etc\n",
    "\n",
    "8. Use the mapping to create updated `y_train` and `y_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, our updated `y_train` and `y_test` are represented by integers. However, using this ordinal encoding (assigning integers to categories) may introduce an inherent order or magnitude between categories that does not exist in the original data. \n",
    "\n",
    "One-hot encoding avoids this issue by representing each category as a separate binary variable, thereby treating them as equally important and removing any ordinal bias. In logistic regression, when dealing with multiple classes, one-hot encoding enables the model to predict probabilities for each class independently. Each class is represented by its own binary variable, and the model learns separate coefficients for each class, allowing it to make predictions across multiple classes.\n",
    "\n",
    "Example:  If `y_train = [0,0,1,2]` then one-hot encoded representation of this vector would be: `[[1,0,0], [1,0,0], [0,1,0], [0,0,1]]`\n",
    "\n",
    "9. Write a function to one-hot encode the labels.\n",
    "\n",
    "10. Call the function to get one-hot encoded labels for both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report first 10 instances of:\n",
    "\n",
    "1. Your original `y_train` and `y_test` arrays.\n",
    "2. Your integer-encoded `y_train` and `y_test` arrays. \n",
    "3. Your one-hot encoded `y_train` and `y_test` arrays. \n",
    "\n",
    "(Note how reporting this can help you understand your arrays properly and verify that they are correctly structured too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to use the above matrices/arrays to implement your classifier and evaluate it on the test set. \n",
    "Please feel free to create as many cells as you need for modularity. \n",
    "\n",
    "Remember there are 5 points for well-commented/structured code. \n",
    "\n",
    "### Implement the following:\n",
    "\n",
    " * Softmax function\n",
    " * Cross-entropy loss function\n",
    " * Batch Gradient Descent function\n",
    " * Prediction function that predicts output class using learned logistic regression (choose the highest probability class)\n",
    " * Evaluation function that calculates classification accuracy on test set \n",
    " * Report plots with no. of iterations/epochs on x-axis and training/validation loss on y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1B: Reflection Questions\n",
    "\n",
    "#### 1. How did normalization help in our case? Why did we need it? \n",
    "\n",
    "$\\color{green}{\\text{Answer:}}$ Double click `here` to answer.\n",
    "#### 2. What function instead of Softmax would we have used if this was a binary classification problem? How does softmax differ from it? \n",
    "\n",
    "$\\color{green}{\\text{Answer:}}$ Double click `here` to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Regularization - Ridge Regression\n",
    "\n",
    "### Problem Introduction\n",
    "\n",
    "This task is intended to familiarise you with regularization in the context of linear regression **(recall that you have implemented Linear Regression in PA1, it will be needed again here)**.\n",
    "\n",
    "While training regression models, it is useful to ensure that there are some constraints on the weights learnt. These constraints are usually defined by some kind of penalty function included in the overall loss function. Generally, penalizing the norm of our weights means that during the training process, our model will prefer smaller values of weights.\n",
    "\n",
    "We shall look at two types of regularization techniques. The first is Ridge Regression:\n",
    "\\begin{equation*}\n",
    "\\mathcal{L} = \\frac{1}{N}(y - X\\mathbf{w})^{2} + \\lambda (\\mathbf{w})^{2}\n",
    "\\end{equation*}\n",
    "\n",
    "The second is Lasso Regression:\n",
    "\\begin{equation*}\n",
    "\\mathcal{L} = \\frac{1}{N}(y - X\\mathbf{w})^{2} + \\lambda |\\mathbf{w}|\n",
    "\\end{equation*}\n",
    "\n",
    "The L2 penalty on the weights penalizes larger values more than smaller ones while the L1 penalty penalizes all kinds of weights equally. The L1 penalty is also used to promote sparsity in the learnt weights (i.e. make certain weights which do not have a strong correlation with the data).\n",
    "\n",
    "Please also note, for regularization to function properly, input **data must be normalized** to have zero mean and unit variance. We will be using StandardScaler() for it.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset for this part is provided in a csv titled `auto_mpg.csv`. \n",
    "\n",
    "### Task\n",
    "\n",
    "Your task is to implement a linear regression model to predict car `displacement` using the feature `mpg`. You will test different values of regularization parameter (`lambd`) to see which value gives the lowest training and testing loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To begin, load the `auto_mpg_dataset.csv` dataset into a pandas DataFrame, visualize it and report its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extract the columns `mpg` and `displacement` and normalize these features by using a builtin function `StandardScaler()`. (You may need to clean the data for Null/NAN values before normalizing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split your data into train and test sets. You may make use of the `train_test_split` function from the scikit-learn library for this. The documentation for this function can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split). Make sure that you use a 80-20 split meaning 80% should be the training set and 20% should be for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You shall now use gradient descent and analytical solution to fit model parameters. To do so, you must complete the `LinearRegression` class provided. This class should work for Ridge Regression (HINT : Gradient Calculations are different). \n",
    "\n",
    "Furthermore, it should be able to compute solutions `analytically` (also provided in sir's notes) as below\n",
    "\n",
    "\\begin{equation*}\n",
    "(X^T X)^{-1} \\cdot (X^T Y)\n",
    "\\end{equation*} \n",
    "\n",
    "as well as via gradient descent. \n",
    "\n",
    "A function to generate the feature matrix for a specified polynomial degree has been provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    A class to perform linear regression\n",
    "\n",
    "    Methods:\n",
    "        __init__(self, lambd, degree)                                                  : Initializes the LinearRegression instance.\n",
    "        _normalize_input(self, x)                                                      :\n",
    "        _generate_X(self, x)                                                           : Generate the matrix X containing samples of data upto the degree specified.\n",
    "                                                                                         Bias term is included (i.e. first column is all ones).\n",
    "        analytical_solution(self, x, y)                                                : Find the analytical solution for model weights which minimizes mean square error\n",
    "        gradient_descent_solution(self, x, y, learning_rate, num_iterations, tol=1e-4) : Find a gradient descent based solution for model weights which minimizes mean square error.\n",
    "    \"\"\"\n",
    "    def __init__(self, lambd, degree):\n",
    "\n",
    "        self.lambd = lambd\n",
    "        self.degree = degree\n",
    "\n",
    "    def _generate_X(self, x):\n",
    "        \"\"\"\n",
    "        Generate the matrix X containing samples of data upto the degree specified.\n",
    "        Bias term is included (i.e. first column is all ones).\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray) : Input data of shape (num_points, 1)\n",
    "\n",
    "        Returns:\n",
    "            X (numpy.ndarray) : Matrix of shape (num_points, degree+1)\n",
    "        \"\"\"\n",
    "        polynomial_features = PolynomialFeatures(degree=self.degree)\n",
    "        X = polynomial_features.fit_transform(x)\n",
    "        return X\n",
    "\n",
    "    def analytical_solution(self, x, y): \n",
    "        \"\"\"\n",
    "        Find the analytical solution for model weights which minimizes mean square error\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray) : x values of data\n",
    "            y (numpy.ndarray) : y values of data\n",
    "\n",
    "        Returns:\n",
    "            w                 : list of optimal weights for regression\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE ##\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def gradient_descent_solution(self, x, y, learning_rate, num_iterations, tol=1e-4):\n",
    "        \"\"\"\n",
    "        Find a gradient descent based solution for model weights which minimizes mean square error.\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray)    : x values of data\n",
    "            y (numpy.ndarray)    : y values of data\n",
    "            learning_rate (float): Learning rate for each gradient descent step\n",
    "            num_iterations (int) : Number of iterations to perform before returning\n",
    "            tol (float)          : value of epsilon s.t. when ||grad(f(x))||_{2} < epsilon, the algorithm terminates\n",
    "\n",
    "        Returns:\n",
    "        w               : list of optimal weights for regression\n",
    "        \"\"\"\n",
    "        ## YOUR CODE HERE ##\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use the LinearRegression class you have implemented above to compute weights using analytical and gradient descent solutions for each value of lambd. Using these computed weights, predict the displacement (this will be Y_predicted). Also plot the curves showing training and testing RMSE`(Y - Y_predicted)^2` for each value of `lambd`. We'll be taking a polynomial of `degree 3 with 100 values of lambda ranging from 0 to 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Plot the output of the model with the least validation RMSE overlaid on top of the original data (mpg vs displacement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What do you notice about the effect of varying $\\lambda$ on dataset (the effect on the training/testing loss)?       \n",
    "$\\color{green}{\\text{Answer:}}$ Double click `here` to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7. If you were to vary `learning rate` on dataset, what effect do you think if would have on the training/testing loss?                                                               \n",
    "$\\color{green}{\\text{Answer:}}$ Double click `here` to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Regularization - Lasso Regression (with sci-kit learn)\n",
    "\n",
    "Now we'll be using the scikit-learn library to perform Lasso regression using different values of alpha to learn the relationship between $y$: fuel average / mpg of different cars and $\\mathbf{x}$: displacement in the `auto_mpg` dataset. You may create your own version of the `generate_X` function from the class implemented above to experiment with models that are non-linear _in features_. Also, do not forget to normalize your input data. A good way to implement all of these in one go is to make a scikit-learn [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). Alpha values list is given.\n",
    "\n",
    "1. Print the coefficients learnt by Lasso model for each value of alpha given. Is there anything particular about the change in coefficients learnt by Lasso regression with different values of alpha?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [10 ,1, 0.1, 0.01, 0.001, 0.0001]\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We will be using the coefficents (computed above) and see which of them provide us with minimum RMSE (same as we computed for ridge regression). Plot the output of the model (Lasso) with the least validation RMSE overlaid on top of the original data. Do not forget to report both training and testing RMSE plots for the values of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **THE END :)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
