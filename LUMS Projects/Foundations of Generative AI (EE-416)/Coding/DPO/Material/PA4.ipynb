{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611135d2",
   "metadata": {},
   "source": [
    "# PA4 - DPO\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will learn how to apply DPO to the SmolLM model you implemented in PA2. Before starting working on this notebook, please make sure to go through the README.md provided as it will intoduce to the concepts relevant to the assignment.\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Follow along with the notebook, filling out the necessary code where instructed.\n",
    "\n",
    "- <span style=\"color: red;\">Read the Submission Instructions, Plagiarism Policy, and Late Days Policy in the attached PDF.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Do not remove any pre-written code.</span>\n",
    "\n",
    "- <span style=\"color: red;\">You must attempt all parts.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c1401",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f87edd",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8511ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# For tokenization and dataset loading\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cfac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb9659",
   "metadata": {},
   "source": [
    "### Initializing device here for future use if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c916a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3e2041",
   "metadata": {},
   "source": [
    "## SmolLM Code\n",
    "\n",
    "As in PA3, you will be using the smolLM model implemented in PA2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77325aee",
   "metadata": {},
   "source": [
    "#### The implementation for the SmolLM model from PA2 has been added below for your convenience. You just need to run the next 5 cells in order to define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ca1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass \n",
    "class smolConfig:\n",
    "    vocab_size = 49152\n",
    "    hidden_size = 576\n",
    "    intermediate_size = 1536\n",
    "    num_hidden_layers = 30\n",
    "    num_heads = 9\n",
    "    kv_heads = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"\n",
    "    Helper function to rotate the left half of a tensor along its final dimension.\n",
    "    \"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"\n",
    "    Applies RoPE on the query and key tensors.\n",
    "    \"\"\"\n",
    "    cos, sin = cos.to(q.device), sin.to(q.device)\n",
    "\n",
    "    # Unsqueexzing to enable broadcasting\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    \n",
    "    return q_embed, k_embed\n",
    "\n",
    "class RotaryEmbedder(nn.Module):\n",
    "    def __init__(self, dim, base):\n",
    "        super().__init__()\n",
    "        # Precompute frequency for sine/cosine embeddings\n",
    "        self.freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        # Generate positions (sequence indices) for the input\n",
    "        pos = torch.arange(x.shape[-2], dtype=torch.long)\n",
    "        # Compute angles for sine and cosine embeddings\n",
    "        angles = torch.einsum(\"p,f->pf\", pos.float(), self.freq).unsqueeze(dim=0)\n",
    "        # Duplicate angles for sine and cosine embeddings\n",
    "        emb = torch.cat((angles, angles), dim=-1)\n",
    "        # Return cosine and sine components of the positional embeddings\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Model dimensions and attention configurations\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.hidden_size // self.num_heads\n",
    "        self.kv_heads = config.kv_heads  # Number of key-value heads\n",
    "        self.rope_theta = 10000.0  # Scaling factor for rotary embeddings\n",
    "\n",
    "        # Linear projections for queries, keys, values, and output\n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "\n",
    "        # Rotary embedding generator\n",
    "        self.rotary_emb = RotaryEmbedder(base=self.rope_theta, dim=self.head_dim)\n",
    "\n",
    "    def _repeat_kv(self, x, n_rep):\n",
    "        batch, num_key_value_heads, slen, head_dim = x.shape\n",
    "        # Expand the number of key-value heads by repeating them\n",
    "        x = x[:, :, None, :, :].expand(\n",
    "            batch, num_key_value_heads, n_rep, slen, head_dim\n",
    "        )\n",
    "        # Reshape to align with the expected multi-head attention format\n",
    "        return x.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, attention_mask=None):\n",
    "        # Input dimensions: (batch_size, seq_len, hidden_size)\n",
    "        b, q, _ = x.size()\n",
    "\n",
    "        # Project input hidden states into queries, keys, and values\n",
    "        q_states = self.q_proj(x)\n",
    "        k_states = self.k_proj(x)\n",
    "        v_states = self.v_proj(x)\n",
    "\n",
    "        # Reshape and transpose for multi-head attention\n",
    "        q_states = q_states.view(b, q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k_states = k_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v_states = v_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute rotary positional embeddings\n",
    "        cos, sin = self.rotary_emb(q_states)\n",
    "        cos = cos.to(q_states.device)\n",
    "        sin = sin.to(q_states.device)\n",
    "        # Apply positional embeddings to queries and keys\n",
    "        q_states, k_states = apply_rotary_pos_emb(q_states, k_states, cos, sin)\n",
    "\n",
    "        # Repeat key and value tensors to match the number of query heads\n",
    "        __kv_groups = self.num_heads // self.kv_heads\n",
    "        k_states = self._repeat_kv(k_states, __kv_groups)\n",
    "        v_states = self._repeat_kv(v_states, __kv_groups)\n",
    "\n",
    "        # Compute attention scores (scaled dot-product attention)\n",
    "        attn_weights = torch.matmul(q_states, k_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Add attention mask (e.g., for causal or padding masking)\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # Normalize attention weights using softmax\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = nn.functional.dropout(attn_weights, 0)\n",
    "\n",
    "        # Compute attention output\n",
    "        attn_output = torch.matmul(attn_weights, v_states)\n",
    "        # Reshape and transpose back to original format\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(b, q, -1)\n",
    "\n",
    "        # Project the attention output back to the hidden size\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        # Return the final attention output\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf7ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        This is the Root Mean Square Normalisation class.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))  # Learnable scaling factor\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate variance along the last dimension (hidden size)\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "\n",
    "        # Normalize and scale\n",
    "        x = x * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        \"\"\"\n",
    "        This is the gated MLP from the LLaMa architecture. Here we use the SiLU acitvation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.activation = nn.modules.activation.SiLU()\n",
    "\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.activation(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "    \n",
    "class LlamaDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        This is the Llama decoder block.\n",
    "        \"\"\"\n",
    "        # Self Attention Module\n",
    "        self.self_attn = GroupedQueryAttention(config)\n",
    "\n",
    "        # FFN Module\n",
    "        self.mlp = MLP(hidden_size=config.hidden_size, intermediate_size=config.intermediate_size)\n",
    "\n",
    "        # Pre Attention and Post Attention normalisation\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # Skip connection cache\n",
    "        \n",
    "        residual = x\n",
    "\n",
    "        # Pre-attention normalisation\n",
    "        x = self.input_layernorm(x)\n",
    "\n",
    "        # A causal attention mask (i.e., decoder can only look at tokens that it has generated thus far)\n",
    "        attention_mask = torch.triu(torch.full((attention_mask.shape[-1], attention_mask.shape[-1]),\n",
    "                                               fill_value=float('-inf')), diagonal=1)\n",
    "        \n",
    "        attention_mask = attention_mask.to(x.device)\n",
    "\n",
    "        # Self-attention block\n",
    "        x = self.self_attn(x=x,attention_mask=attention_mask)\n",
    "        x += residual\n",
    "\n",
    "        # Skip connection cache for MLP\n",
    "        residual = x\n",
    "\n",
    "        # Pre-MLP normalisation\n",
    "        x = self.post_attention_layernorm(x)\n",
    "\n",
    "        # MLP block\n",
    "        x = self.mlp(x)\n",
    "        x += residual\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45248cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class smolModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # embedding layer which maps each token to a vector embedding\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.hidden_size\n",
    "        )\n",
    "\n",
    "        # Stack of decoder layers (LlamaDecoder) defined by the configuration\n",
    "        self.layers = nn.ModuleList([\n",
    "            LlamaDecoder(config) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "        # RMSNorm: final layer normalization applied to hidden states\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "        x = inputs_embeds\n",
    "\n",
    "        # Pass embeddings through each decoder layer\n",
    "        for i, decoder_layer in enumerate(self.layers):\n",
    "            layer_outputs = decoder_layer(\n",
    "                x,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            x = layer_outputs\n",
    "\n",
    "        # Final normalisation\n",
    "        x = self.norm(x)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "class smolLM(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the Language Model. \n",
    "    It passes the embeddings from the SmolLM backbone into a LM head.\n",
    "    The LM head generates logits over the space of the entire vocabulary for next word prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # SmolLM backbone which generates the contextualised embeddings for the input tokens\n",
    "        self.model = smolModel(config)\n",
    "        # The LM head which maps embeddings to logits over the vocabulary\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        # weights between LM head and the token_embedding layer are shared in the SmolLM architecture\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        # lm_head shares weights with the embedding layer\n",
    "        self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Input tokens are passed to the SmolLM backbone\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        # embeddings corresponding to each input token => (batch_size, seq_len, emb_dim)\n",
    "        x = outputs\n",
    "\n",
    "        # pass the embeddings through the LM head\n",
    "        logits = self.lm_head(x).float()\n",
    "        return {'logits': logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b63ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __generate(model, inputs, num_tokens, tokenizer, max_length=50):\n",
    "    \"\"\"\n",
    "    A basic greedy approach for text generation.\n",
    "    \"\"\"\n",
    "    collect = []\n",
    "    for _ in range(num_tokens):\n",
    "        output = model(**inputs)\n",
    "        output_id = torch.argmax(output['logits'][0, -1]).item()\n",
    "        collect.append(output_id)\n",
    "        if output_id == tokenizer.eos_token_id or len(collect) >= max_length:\n",
    "            break\n",
    "        # Update input_ids and attention_mask\n",
    "        new_token = torch.tensor([output_id], device=inputs['input_ids'].device)\n",
    "        inputs['input_ids'] = torch.cat([inputs['input_ids'][0], new_token]).unsqueeze(0)\n",
    "        inputs['attention_mask'] = F.pad(inputs['attention_mask'], (0, 1), value=1)\n",
    "    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(collect))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ab8861",
   "metadata": {},
   "source": [
    "## Preference Optimization\n",
    "\n",
    "In a typical LLM training pipeline, after pre-training the model, we first perform supervised finetuning, which you have already done in PA3 with LoRA. For this assignment, we will use the fully finetuned model you used as a baseline for comparison in PA3. The weights are accesible through the drive link [Finetuned Base Model Weights](https://drive.google.com/drive/folders/1eIflNAp9UE4Fm8ZrBAzjDPsOCs-s_O55?usp=sharing)\n",
    "\n",
    "Once our model is finetuned, we want to align our model using preference optimization. As you might have learned in class, this was typically done using Principle Policy Optimization (PPO). However, once Direct Preference Optimization (DPO) was introduced, it has been shown to be very effective and is much simpler to implement.\n",
    "\n",
    "The main difference between PPO and DPO lies in the use of a reward model. In PPO, a separate reward model is trained to provide feedback to the policy (the language model) during training. The reward model is used to compute a reward signal that guides the policy's updates. In contrast, DPO directly optimizes the policy based on preference data without the need for a separate reward model. This simplifies the training process and reduces the complexity of the overall system.\n",
    "\n",
    "For this assignment, we will only implement DPO. However, I encourage you to look into PPO as well, as reward modelling is a very important concept in RLHF. In fact, DeepSeek-v3 uses GRPO, which is essentially a variation of PPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524e34a",
   "metadata": {},
   "source": [
    "### DPO\n",
    "\n",
    "**Direct Preference Optimization (DPO)** is a technique used to align language models with human preferences without needing reinforcement learning or a separate reward model.\n",
    "\n",
    "Instead of training a reward model, DPO directly learns from data where humans have ranked one response as better than another for the same prompt. This is done using a contrastive loss: the model is trained to assign higher likelihood to the preferred (chosen) response than the rejected one.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Given a dataset of triplets:\n",
    "- `prompt`\n",
    "- `chosen response` (preferred by humans)\n",
    "- `rejected response` (less preferred)\n",
    "\n",
    "The model compares its confidence (log-likelihood) in both responses, and adjusts itself to prefer the better one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59cbc1d",
   "metadata": {},
   "source": [
    "#### Loading Models [5 points]\n",
    "\n",
    "We need to load a policy model and a reference model. Both models will be initiliazed from the finetuned model weights. We need to freeze the reference model so that it does not get updated during training. The policy model will be updated using DPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe5539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config\n",
    "config = smolConfig()\n",
    "\n",
    "# TODO: Initiliaze the policy model and load the weights\n",
    "policy_model = ...\n",
    "\n",
    "# TODO: Initialize the reference model and load the weights\n",
    "reference_model = ...\n",
    "\n",
    "reference_model.eval()\n",
    "\n",
    "# TODO: Freeze all parameters in the reference model\n",
    "...\n",
    "\n",
    "# Move the models to the appropriate device (GPU or CPU)\n",
    "policy_model.to(device)\n",
    "reference_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac3bd7",
   "metadata": {},
   "source": [
    "Next, we will need to freeze part of the policy model to make training feasible on your devices. You just need to run the code in this section. Before you ask on slack, you are allowed to change the number of layers to freeze to reduce training time, debug or for any other reason. You can also comment out the code block entirely should you wish to train the entire model. However, you will be graded on whether your preference accuracy increases at the end, so keep that in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dddfbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_freeze = 25\n",
    "\n",
    "for name, param in policy_model.named_parameters():\n",
    "    if name.startswith(\"model.embed_tokens\"):\n",
    "        param.requires_grad = False\n",
    "    elif name.startswith(\"model.norm\"):\n",
    "        param.requires_grad = False\n",
    "    elif name.startswith(\"lm_head\"):\n",
    "        param.requires_grad = True\n",
    "    elif name.startswith(\"model.layers.\"):\n",
    "        layer_num = int(name.split(\".\")[2])\n",
    "        if layer_num < layers_to_freeze:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca70c8",
   "metadata": {},
   "source": [
    "Checking number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = sum(p.numel()\n",
    "                       for p in policy_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in policy_model.parameters())\n",
    "print(\n",
    "    f\"Trainable params: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61fa8da",
   "metadata": {},
   "source": [
    "Do not worry if this number is a bit high. We will adjust the dataset size to reduce training time later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fea974",
   "metadata": {},
   "source": [
    "#### Loading and Processing Dataset [15 points]\n",
    "\n",
    "We load in the dataset and tokenizer and modify the dataset to fit our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e45200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dpo_dataset = load_dataset(\"Dahoas/full-hh-rlhf\")\n",
    "\n",
    "# Use only a small subset for fast training\n",
    "data = dpo_dataset[\"train\"].shuffle(seed=42)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
    "\n",
    "# The tokenizer does not have a defined padding token, so we initialize our own as the EoS token.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a534088e",
   "metadata": {},
   "source": [
    "To stabilize our training, we want to eliminate examples that may be redundant. Since we don't have preference scores in this dataset, you must think of a way to shortlist useful datapoints yourself. You can do this by analyzing the length of responses, looking for common phrases that may indicate weak choices, etc. You may also use a reward model, but that may prove to be too much work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ee4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the function\n",
    "def is_strong_preference(example):\n",
    "    \"\"\"\n",
    "    Function to filter out weak preferences from the dataset.\n",
    "    \n",
    "    Arguments:\n",
    "        example: A dictionary containing the prompt, chosen, and rejected completions.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the example is a strong preference, False otherwise.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "strong_data = data.filter(is_strong_preference)\n",
    "\n",
    "print(\n",
    "    f\"Filtered down to {len(strong_data)} / {len(data)} strong examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45090ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you can reduce the size for debugging but you should use all 1000 for the final submission\n",
    "split_size = 1000\n",
    "\n",
    "split = strong_data.select(range(split_size)).train_test_split(test_size=0.1, seed=42)\n",
    "train_data = split[\"train\"]\n",
    "test_data = split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46441907",
   "metadata": {},
   "source": [
    "Yes, you are allowed to change the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b1273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, raw_data):\n",
    "        self.data = raw_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            \"prompt\": item[\"prompt\"],\n",
    "            \"chosen\": item[\"chosen\"],\n",
    "            \"rejected\": item[\"rejected\"]\n",
    "        }\n",
    "\n",
    "\n",
    "train_loader = DataLoader(DPODataset(train_data), batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(DPODataset(test_data), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c15865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the function\n",
    "def tokenize_pairs(prompts, responses):\n",
    "    \"\"\"\n",
    "    Tokenizes pairs of prompts and responses using the tokenizer.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6fab61",
   "metadata": {},
   "source": [
    "#### Loss Function [35 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d074df",
   "metadata": {},
   "source": [
    "The **DPO loss function** is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{DPO}} = -\\log\\left( \\sigma\\left( \\beta \\left[ (\\log \\pi_\\theta(y_c|x) - \\log \\pi_\\theta(y_r|x)) - (\\log \\pi_{\\text{ref}}(y_c|x) - \\log \\pi_{\\text{ref}}(y_r|x)) \\right] \\right) \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ x $: the prompt\n",
    "- $ y_c $: the chosen response\n",
    "- $ y_r $: the rejected response\n",
    "- $ \\pi_\\theta $: the current policy model\n",
    "- $ \\pi_{\\text{ref}} $: the frozen reference model\n",
    "- $ \\beta $: a temperature-like hyperparameter (typically 0.1–0.5)\n",
    "- $ \\sigma $: the sigmoid function\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- If the current policy assigns higher log-probability to the chosen response (relative to rejected) more than the reference did, the loss will decrease.\n",
    "- This means the policy is improving beyond the baseline set by the reference model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a58217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the function\n",
    "def compute_logprob(model, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes the average log-probability per sample in a batch for causal language modeling.\n",
    "\n",
    "    Args:\n",
    "        model: A language model that outputs logits for next-token prediction.\n",
    "        input_ids (Tensor): Tensor of shape (batch_size, seq_len) containing token IDs.\n",
    "        attention_mask (Tensor): Tensor of shape (batch_size, seq_len) indicating non-padding tokens.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size,) containing average log-probabilities for each sample.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO: Define the function\n",
    "def dpo_loss(policy_model, ref_model, chosen_inputs, rejected_inputs, beta=0.1):\n",
    "    \"\"\"\n",
    "    Computes the Direct Preference Optimization (DPO) loss for a batch.\n",
    "\n",
    "    Args:\n",
    "        policy_model: The trainable language model to align with human preferences.\n",
    "        ref_model: The frozen reference model (typically the SFT baseline).\n",
    "        chosen_inputs (dict): Tokenized inputs of (prompt + chosen response).\n",
    "        rejected_inputs (dict): Tokenized inputs of (prompt + rejected response).\n",
    "        beta (float): Scaling factor for the preference margin (contrast strength).\n",
    "\n",
    "    Returns:\n",
    "        A tuple of:\n",
    "            - Scalar loss averaged over the batch\n",
    "            - Logits difference vector (useful for accuracy or debugging)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe887b8",
   "metadata": {},
   "source": [
    "#### Training [20 points]\n",
    "Finally, we can define the training loop. You must implement the training loop yourself. Make sure to log at least the following:\n",
    "1. Running loss\n",
    "2. Batch loss\n",
    "3. Batch Logits Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70729632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set models to appropriate mode\n",
    "policy_model.train()\n",
    "reference_model.eval()\n",
    "\n",
    "# Optimizer (you can change the lr and/or optimizer if you wish)\n",
    "optimizer = torch.optim.Adam(policy_model.parameters(), lr=3e-4)\n",
    "\n",
    "# Initialize stats\n",
    "all_losses = []\n",
    "all_logits_diff = []\n",
    "running_loss = 0.0\n",
    "\n",
    "# TODO: Write the training loop\n",
    "for step, batch in enumerate(train_loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d6ad9",
   "metadata": {},
   "source": [
    "Plot the loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497abb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(all_losses, label='Loss')\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aafab9d",
   "metadata": {},
   "source": [
    "Plot the logits difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d706cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(all_logits_diff, label='Logits Difference')\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Logits Difference')\n",
    "plt.title('Logits Difference Over Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840c6d8",
   "metadata": {},
   "source": [
    "#### Evaluation [15 points]\n",
    "\n",
    "Evaluating the model to see the fruits of our labour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the function\n",
    "def compute_accuracy(logits_diff):\n",
    "    \"\"\"\n",
    "    Computes accuracy: how often the model prefers the chosen response (logits_diff > 0)\n",
    "    \n",
    "    Args:\n",
    "        logits_diff (Tensor): Vector of logits differences for a batch\n",
    "    \n",
    "    Returns:\n",
    "        accuracy (float): Accuracy as a decimal (between 0 and 1)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e6bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the function\n",
    "def evaluate_dpo_accuracy(model, loader, beta=0.1):\n",
    "    \"\"\"\n",
    "    Evaluates how often the model prefers the chosen response over the rejected one.\n",
    "    Returns: accuracy as a float (0 to 1)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "ref_acc = evaluate_dpo_accuracy(reference_model, test_loader)\n",
    "policy_acc = evaluate_dpo_accuracy(policy_model, test_loader)\n",
    "\n",
    "print(f\"Reference Model Accuracy: {ref_acc * 100:.2f}%\")\n",
    "print(f\"Policy Model Accuracy:    {policy_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a613fa43",
   "metadata": {},
   "source": [
    "Next, you must write code to show a few comparisons between the reference (baseline) and the policy model. Randomly sample 3 prompts from the test set and generate responses from both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display sample inputs and outputs from each model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa77a1",
   "metadata": {},
   "source": [
    "Comment on your observations:\n",
    "\n",
    "(Answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7e709",
   "metadata": {},
   "source": [
    "#### Observations and Questions [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f7452",
   "metadata": {},
   "source": [
    "Q1. Comment on the difference between the accuracy of the policy model and that of the reference model. In the case of this assignment, state at least two factors that may be holding back the policy model from having an even higher accuracy. \n",
    "\n",
    "(Answer here)\n",
    "\n",
    "Q2. What does the β parameter in DPO control? What might happen if it is set too high or too low?\n",
    "\n",
    "(Answer here)\n",
    "\n",
    "Q3. What would happen if you mistakenly used the reference model's log-probs in place of the policy model during training?\n",
    "\n",
    "(Answer here)\n",
    "\n",
    "Q4. Suppose a batch contains a rejected response that is significantly longer than the chosen one. How might this affect training? How could you address this?\n",
    "\n",
    "(Answer here)\n",
    "\n",
    "Q5. Briefly explain one problem you ran into while training your model and explain how you solved it. This can be a small issue or a problem you needed to spend a lot of time to fix.\n",
    "\n",
    "(Answer here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
