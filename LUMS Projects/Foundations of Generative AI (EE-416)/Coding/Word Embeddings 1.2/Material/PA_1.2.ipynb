{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA1.2 - Word Embeddings\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will learn how to train your own word embeddings using LSTMs, then explore some of the fun things you can do with them.\n",
    "\n",
    "Word Embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
    "\n",
    "For reference and additional details, please go throught the following resources:\n",
    "\n",
    "1) Chapter 6 of [the SLP3 book](https://web.stanford.edu/~jurafsky/slp3)\n",
    "2) This [nice writeup by Jay Alammar](https://jalammar.github.io/illustrated-word2vec/).\n",
    "3) [LSTMs Basics](https://medium.com/linagoralabs/next-word-prediction-a-complete-guide-d2e69a7a09e6)\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Follow along with the notebook, filling out the necessary code where instructed.\n",
    "\n",
    "- <span style=\"color: red;\">Read the Submission Instructions, Plagiarism Policy, and Late Days Policy in the attached PDF.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Do not remove any pre-written code.</span>\n",
    "\n",
    "- <span style=\"color: red;\">You must attempt all parts.</span>\n",
    "\n",
    "- <span style=\"color: red;\">You are only required to submit your ipynb file with the following format: {roll_number}.ipynb.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 -  Learning Word Representations with LSTM Language Models [50 points]\n",
    "\n",
    "Now to spice things up.\n",
    "\n",
    "In this part, you will frame the problem of learning word representations as a sequential prediction task using an LSTM-based Language Model (LSTM-LM). The model will take sequential input data and predict the next token in the sequence based on its learned representation of the context.\n",
    "\n",
    "Model Architecture\n",
    "\n",
    "- Input Layer: The model takes a one-hot encoded representation of the input tokens with a shape of `(batch_size, vocab_size)` for each time step.\n",
    "- LSTM Layers: The model employs gated mechanisms `(input, forget, and output gates)` to capture long-term dependencies in the data. The LSTM cell computes hidden and cell states iteratively across the sequence.\n",
    "- Output Layer: The hidden state of the LSTM is projected to the vocabulary size using a fully connected layer, followed by a softmax function to produce a probability distribution over the vocabulary.\n",
    "- Learning Parameters: Gradients are computed using backpropagation through time `(BPTT)`, and parameters are updated using gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T19:01:15.505530Z",
     "iopub.status.busy": "2025-01-18T19:01:15.505252Z",
     "iopub.status.idle": "2025-01-18T19:01:15.510373Z",
     "shell.execute_reply": "2025-01-18T19:01:15.509180Z",
     "shell.execute_reply.started": "2025-01-18T19:01:15.505505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import in the libraries\n",
    "# Note: you are NOT allowed to use any other libraries or functions outside of these\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T19:04:54.913038Z",
     "iopub.status.busy": "2025-01-18T19:04:54.912685Z",
     "iopub.status.idle": "2025-01-18T19:04:54.924506Z",
     "shell.execute_reply": "2025-01-18T19:04:54.923439Z",
     "shell.execute_reply.started": "2025-01-18T19:04:54.913009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ready the dataset again\n",
    "with open(\"/kaggle/input/The Fellowship of the Ring.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "print(data[:200])\n",
    "len(data)\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, data: str):\n",
    "        data = re.sub(r\"[,!?;-]\", \".\", data)\n",
    "        tokens = word_tokenize(data)\n",
    "        tokens = [token for token in tokens if token.isalpha() or token == '.']\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        self.data = data\n",
    "        self.tokens = tokens\n",
    "        self.vocab = sorted(set(tokens))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.stoi = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.itos = {i: word for i, word in enumerate(self.vocab)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "# Instantiate the Dataset class\n",
    "dataset = Dataset(data)\n",
    "print(f\"Number of tokens in dataset: {len(dataset)}\")\n",
    "print(f\"Vocabulary size: {dataset.vocab_size}\")\n",
    "\n",
    "\n",
    "def get_windows(\n",
    "        data: List[Union[str, int]], \n",
    "        ctx_size: int\n",
    "    ):\n",
    "    outside_words = []\n",
    "    center_words = []\n",
    "\n",
    "    # Iterate over the data with a sliding window\n",
    "    for i in range(ctx_size, len(data) - ctx_size):\n",
    "        # The center word is the current word\n",
    "        center_words.append(data[i])\n",
    "        # Context words are the words around the center word\n",
    "        context = data[i - ctx_size:i] + data[i + 1:i + ctx_size + 1]\n",
    "        outside_words.append(context)\n",
    "    \n",
    "    return outside_words, center_words\n",
    "\n",
    "# Prepare context words and target words\n",
    "context_words, target_words = get_windows(\n",
    "    dataset.tokens[:2000], \n",
    "    ctx_size=3\n",
    ")\n",
    "\n",
    "# Encode the context and target words\n",
    "encode = lambda x: [dataset.stoi[c] for c in x] if isinstance(x, list) else dataset.stoi[x]\n",
    "context_words = [encode(cw) for cw in context_words]\n",
    "target_words = [encode(tw) for tw in target_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our Dataset\n",
    "\n",
    "The primary objective is to prepare the data by processing the context words and target words for input to the model. This involves:\n",
    "\n",
    "- One-Hot Encoding: Each word is converted into a one-hot encoded vector, where the vector length corresponds to the size of the vocabulary. One-hot encoding allows us to represent each word as a unique vector where all positions are 0 except for the position corresponding to the word.\n",
    "- Context Windows: A context window consists of a set of surrounding words (context words) and the target word in the middle. The context window is used as the input to predict the target word using the LSTM model.\n",
    "- Data Representation: The input data (X) consists of the context words. We will represent each context word as a sequence of one-hot vectors. Each context window will be a sequence of vectors, where each vector represents a word in the context. The target data (y) consists of the target words. Each target word will also be represented by a one-hot encoded vector.\n",
    "\n",
    "--- \n",
    "\n",
    "In the cell below, you will define\n",
    "\n",
    "- A function to  `one_hot_encode` a single word index. The function will take a word index and the vocabulary size as inputs and return a one-hot encoded vector. The vector should have a length equal to the vocabulary size, with a 1 at the index corresponding to the word and 0s elsewhere.\n",
    "\n",
    "- A function to do the following operation: `context_words_to_vector` will convert the context words into a sequence of one-hot vectors for each context window. This function will convert each word index in the context window into a one-hot encoded vector. The output will be a matrix where each row is a one-hot vector for a word in the context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T19:06:40.363848Z",
     "iopub.status.busy": "2025-01-18T19:06:40.363448Z",
     "iopub.status.idle": "2025-01-18T19:06:41.707140Z",
     "shell.execute_reply": "2025-01-18T19:06:41.706043Z",
     "shell.execute_reply.started": "2025-01-18T19:06:40.363803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if '<unk>' not in dataset.stoi:\n",
    "    unk_idx = len(dataset.stoi)\n",
    "    dataset.stoi['<unk>'] = unk_idx\n",
    "    \n",
    "    if not hasattr(dataset, 'itos'):\n",
    "        dataset.itos = [] \n",
    "    dataset.itos.append('<unk>') \n",
    "\n",
    "# Modified encode function to handle unknown words\n",
    "encode = lambda x: [dataset.stoi[c] if c in dataset.stoi else dataset.stoi['<unk>'] for c in x] if isinstance(x, list) else (dataset.stoi[x] if x in dataset.stoi else dataset.stoi['<unk>'])\n",
    "\n",
    "# Encode context and target words using the modified encode function\n",
    "context_words = [encode(cw) for cw in context_words]\n",
    "target_words = [encode(tw) for tw in target_words]\n",
    "\n",
    "\n",
    "# Define one-hot encoding function with bounds checking\n",
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word index.\n",
    "    Args:\n",
    "        idx: index of the word in the vocabulary.\n",
    "        vocab_size: total number of words in the vocabulary.\n",
    "    Returns:\n",
    "        One-hot encoded vector of length vocab_size.\n",
    "    \"\"\"\n",
    "     ## Your code here\n",
    "\n",
    "# Convert context words into a sequence of one-hot encoded vectors for each context window\n",
    "def context_words_to_sequence(context, vocab_size):\n",
    "    \"\"\"\n",
    "    Converts context words to a sequence of one-hot vectors.\n",
    "    Args:\n",
    "        context: list of word indices representing the context.\n",
    "        vocab_size: size of the vocabulary.\n",
    "    Returns:\n",
    "        A matrix where each row is a one-hot vector for a context word.\n",
    "    \"\"\"\n",
    "     ## Your code here\n",
    "\n",
    "# Prepare the input data (X) as sequences of one-hot vectors\n",
    "vocab_size = dataset.vocab_size\n",
    "X = np.array([context_words_to_sequence(cw, vocab_size) for cw in context_words])\n",
    "\n",
    "# Prepare the target data (y) as one-hot encoded vectors\n",
    "y = np.array([one_hot_encode(t, vocab_size) for t in target_words])\n",
    "\n",
    "# Check shapes to ensure the data is correctly formatted for LSTM\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# LSTM-based Language Model from Scratch\n",
    "\n",
    "In this task, we are tasked with creating our own LSTM-based Language Model from scratch. The model will be designed to predict the next word in a sequence, based on the context words around it. This is a multiclass classification problem, where the objective is to predict the probability distribution of the target word across the vocabulary.\n",
    "\n",
    "The **LSTM (Long Short-Term Memory)** network is a type of **Recurrent Neural Network (RNN)** that is particularly effective at handling sequential data, such as text. In our case, the LSTM will learn to model the relationship between context words and the target word, while maintaining memory over time to capture long-range dependencies.\n",
    "\n",
    "### Objective\n",
    "The goal of this task is to complete the implementation of the **LSTM-based Language Model (LSTMLM)** class, which will perform the following:\n",
    "\n",
    "1. **Forward Pass**: The LSTM network processes a sequence of context words, computes the hidden state and cell state at each timestep, and outputs a prediction for the target word at each timestep.\n",
    "2. **Backward Pass**: The gradients of the model's parameters are computed using backpropagation through time (BPTT). This involves computing the gradient of the loss with respect to each parameter (weights and biases), and using these gradients to update the model's parameters.\n",
    "3. **Softmax Output**: Since this is a multiclass classification problem, the model will output a probability distribution over the vocabulary for each target word prediction. The **softmax** function is used to convert the output logits into probabilities.\n",
    "4. **Parameter Update**: After computing the gradients during the backward pass, we use an optimization method (e.g., gradient descent) to update the parameters.\n",
    "\n",
    "### Steps to Implement:\n",
    "\n",
    "#### 1. Define the LSTM Architecture:\n",
    "LSTM has four gates:\n",
    "- **Input gate**: Controls how much of the new information is written to the cell state.\n",
    "- **Forget gate**: Decides how much of the previous cell state should be retained.\n",
    "- **Output gate**: Determines the next hidden state based on the cell state.\n",
    "- **Cell candidate**: A potential update to the cell state based on the input and previous hidden state.\n",
    "\n",
    "The equations for the LSTM's operations at each timestep are as follows:\n",
    "\n",
    "- **Input gate**: \n",
    "  $$i = \\sigma(W_i \\cdot x + U_i \\cdot h_{\\text{prev}} + b_i)$$\n",
    "  \n",
    "- **Forget gate**: \n",
    "  $$f = \\sigma(W_f \\cdot x + U_f \\cdot h_{\\text{prev}} + b_f)$$\n",
    "  \n",
    "- **Output gate**: \n",
    "  $$o = \\sigma(W_o \\cdot x + U_o \\cdot h_{\\text{prev}} + b_o)$$\n",
    "  \n",
    "- **Cell candidate**: \n",
    "  $$\\tilde{c} = \\tanh(W_c \\cdot x + U_c \\cdot h_{\\text{prev}} + b_c)$$\n",
    "  \n",
    "- **Cell state**: \n",
    "  $$c = f \\cdot c_{\\text{prev}} + i \\cdot \\tilde{c}$$\n",
    "  \n",
    "- **Hidden state**: \n",
    "  $$h = o \\cdot \\tanh(c)$$\n",
    "\n",
    "The output of the LSTM at each timestep is:\n",
    "$$\n",
    "y = W_y \\cdot h + b_y\n",
    "$$\n",
    "where \\(W_y\\) and \\(b_y\\) are the output weight matrix and bias vector.\n",
    "\n",
    "#### 2. Forward Pass:\n",
    "- Process each word in the input sequence one by one using the LSTM.\n",
    "- At each timestep, compute the output of the LSTM (the hidden state and cell state), and make predictions for the next word.\n",
    "\n",
    "#### 3. Backward Pass:\n",
    "- Using the softmax output and the true labels, compute the loss and the gradients for the parameters in the LSTM. This is done using **backpropagation through time (BPTT)**.\n",
    "\n",
    "#### 4. Loss Function:\n",
    "- The loss function for the language model is the **cross-entropy loss**, which is appropriate for multiclass classification. It is computed as the negative log probability of the correct word given the predicted distribution.\n",
    "\n",
    "#### 5. Optimization:\n",
    "- After computing the gradients, update the parameters using an optimization algorithm (e.g., gradient descent or its variants).\n",
    "\n",
    "### Code Structure:\n",
    "\n",
    "#### LSTMLM Class:\n",
    "The class contains methods for forward and backward passes, as well as parameter updates:\n",
    "- `forward_step`: Computes the output at each timestep, including the hidden and cell states.\n",
    "- `forward`: Computes the outputs for the entire input sequence.\n",
    "- `backward`: Computes gradients with respect to the loss and updates the model parameters.\n",
    "- `update`: Updates the model's parameters using gradient descent.\n",
    "- `fit`: Trains the model for a given number of epochs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "    </summary>\n",
    "    <p>\n",
    "        - You might find it helpful to reference the tutorial notebook for LSTMs at \n",
    "        <a href=\"https://medium.com/linagoralabs/next-word-prediction-a-complete-guide-d2e69a7a09e6\" target=\"_blank\">LSTMs Basics</a>.\n",
    "    </p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T19:11:43.609220Z",
     "iopub.status.busy": "2025-01-18T19:11:43.608818Z",
     "iopub.status.idle": "2025-01-18T19:11:43.635673Z",
     "shell.execute_reply": "2025-01-18T19:11:43.634390Z",
     "shell.execute_reply.started": "2025-01-18T19:11:43.609189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(22) #don't change the seed\n",
    "\n",
    "def softmax(x):\n",
    "    ## Your code here\n",
    "    pass\n",
    "\n",
    "def sigmoid(x):\n",
    "    ## Your code here\n",
    "    pass\n",
    "\n",
    "class LSTMLM:\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Initialize LSTM parameters with correct dimensions\n",
    "        ## Your code here\n",
    "        \n",
    "        ## --\n",
    "\n",
    "    def forward_step(self, x, h_prev, c_prev):\n",
    "        '''\n",
    "        Performs forward pass for a single time step\n",
    "        '''\n",
    "        ## Your code here\n",
    "        \n",
    "        ## --\n",
    "        return y, h, c, cache\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass for the entire sequence\n",
    "        '''\n",
    "        ## Your code here\n",
    "\n",
    "        ## --\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        '''\n",
    "        Backward pass to compute gradients\n",
    "        '''\n",
    "        ## Your code here\n",
    "\n",
    "        ## --\n",
    "\n",
    "    def update(self, lr):\n",
    "        '''\n",
    "        Updates the model parameters using the computed gradients\n",
    "        '''\n",
    "        ## Your code here\n",
    "\n",
    "        ## --\n",
    "\n",
    "    def fit(self, x, y, epochs=10, lr=0.01):\n",
    "        '''\n",
    "        Train the LSTM model\n",
    "        '''\n",
    "        ## Your code here\n",
    "\n",
    "        ## --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model and getting the Embeddings\n",
    "\n",
    "Run the cell below to train your model, and plot the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### YOU ARE STRICTLY PROHIBITED FROM CHANGING THIS CELL !!!\n",
    "\n",
    "np.random.seed(22) #don't change the seed\n",
    "\n",
    "X_seq = X.reshape(X.shape[0], X.shape[1], X.shape[2])  # Shape: (batch_size, seq_len, vocab_size)\n",
    "# Initialize model\n",
    "model = LSTMLM(vocab_size=8445, hidden_dim=100)\n",
    "# Train the model\n",
    "losses = model.fit(X_seq, y, epochs=50, lr=1e-4)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss curve for LSTM Language Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've hopefully seen the model improving, we can extract its weight matrices (Wi for input embeddings and Wf for forget gate embeddings) to use as word embeddings.\n",
    "\n",
    "We have the choice of using either the input embeddings `(Wi)` or the forget gate embeddings `(Wf)`, or we could do something more experimental and take the average of both to analyze the word relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    # Compute cosine similarity between two vectors\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# For LSTM, we'll use the input embeddings (Wi) as our word embeddings\n",
    "# We can also incorporate other matrices but Wi is most directly comparable to the original model's W1\n",
    "embedding_matrix = model.Wi  # Using the input embeddings\n",
    "\n",
    "# Compare character relationships\n",
    "print(\"Character Relationships:\")\n",
    "print(f\"Frodo-Sam Similarity: {cosine_similarity(embedding_matrix[dataset.stoi['frodo']], embedding_matrix[dataset.stoi['sam']]):.4f}\")\n",
    "print(f\"Gandalf-White Similarity: {cosine_similarity(embedding_matrix[dataset.stoi['gandalf']], embedding_matrix[dataset.stoi['white']]):.4f}\")\n",
    "print(f\"Mordor-Frodo Similarity: {cosine_similarity(embedding_matrix[dataset.stoi['mordor']], embedding_matrix[dataset.stoi['frodo']]):.4f}\")\n",
    "print(f\"Shire-Aragorn Similarity: {cosine_similarity(embedding_matrix[dataset.stoi['shire']], embedding_matrix[dataset.stoi['aragorn']]):.4f}\")\n",
    "\n",
    "# You can also analyze the forget gate embeddings to see relationship patterns\n",
    "forget_embeddings = model.Wf\n",
    "print(\"\\nForget Gate Relationships:\")\n",
    "print(f\"Frodo-Sam Similarity (Forget Gate): {cosine_similarity(forget_embeddings[dataset.stoi['frodo']], forget_embeddings[dataset.stoi['sam']]):.4f}\")\n",
    "print(f\"Gandalf-White Similarity (Forget Gate): {cosine_similarity(forget_embeddings[dataset.stoi['gandalf']], forget_embeddings[dataset.stoi['white']]):.4f}\")\n",
    "print(f\"Mordor-Frodo Similarity (Forget Gate): {cosine_similarity(forget_embeddings[dataset.stoi['mordor']], forget_embeddings[dataset.stoi['frodo']]):.4f}\")\n",
    "print(f\"Shire-Aragorn Similarity (Forget Gate): {cosine_similarity(forget_embeddings[dataset.stoi['shire']], forget_embeddings[dataset.stoi['aragorn']]):.4f}\")\n",
    "\n",
    "def plot_word_similarities(words, embedding_matrix):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    similarities = np.zeros((len(words), len(words)))\n",
    "    \n",
    "    for i, word1 in enumerate(words):\n",
    "        for j, word2 in enumerate(words):\n",
    "            similarities[i, j] = cosine_similarity(\n",
    "                embedding_matrix[dataset.stoi[word1]],\n",
    "                embedding_matrix[dataset.stoi[word2]]\n",
    "            )\n",
    "    \n",
    "    plt.imshow(similarities, cmap='coolwarm')\n",
    "    plt.xticks(range(len(words)), words, rotation=45)\n",
    "    plt.yticks(range(len(words)), words)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Word Similarity Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "key_words = ['frodo', 'sam', 'gandalf', 'white', 'mordor', 'shire', 'aragorn']\n",
    "plot_word_similarities(key_words, embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about whether these performed better than the previous ones, and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**  Based on the cosine similarity results and the word similarity matrix visualization, which word pairs have the highest similarity, and what insights can we draw from this in terms of the model's understanding of word relationships?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparision with Word2Vec\n",
    "\n",
    "### Now, since you are successfully implemented a LSTM model, lets reflect back to PA1.1, where you trained a word2vec model to learn the embeddings.\n",
    "\n",
    "In this part load the learned embeddings from PA1.1 and plot a heatmap similar to the one plotted above. After plotting, make a comparision of the heatmaps between LSTM and word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load embeddings in PA1.1\n",
    "embedding_matrix = # word2vec embeddings\n",
    "\n",
    "\n",
    "# Compare character relationships\n",
    "print(\"Character Relationships:\")\n",
    "print(f\"Frodo-Sam Similarity: {cosine_similarity(embedding_matrix[dataset.stoi['frodo']], embedding_matrix[dataset.stoi['sam']]):.4f}\")\n",
    "print(f\"Gandalf-White Similarity: {cosine_similarity(embedding_matrix[dataset.stoi['gandalf']], embedding_matrix[dataset.stoi['white']]):.4f}\")\n",
    "print(f\"Mordor-Frodo Similarity: {cosine_similarity(embedding_matrix[dataset.stoi['mordor']], embedding_matrix[dataset.stoi['frodo']]):.4f}\")\n",
    "print(f\"Shire-Aragorn Similarity: {cosine_similarity(embedding_matrix[dataset.stoi['shire']], embedding_matrix[dataset.stoi['aragorn']]):.4f}\")\n",
    "\n",
    "# Visualization of embeddings (optional)\n",
    "def plot_word_similarities(words, embedding_matrix):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    similarities = np.zeros((len(words), len(words)))\n",
    "    \n",
    "    for i, word1 in enumerate(words):\n",
    "        for j, word2 in enumerate(words):\n",
    "            similarities[i, j] = cosine_similarity(\n",
    "                embedding_matrix[dataset.stoi[word1]],\n",
    "                embedding_matrix[dataset.stoi[word2]]\n",
    "            )\n",
    "    \n",
    "    plt.imshow(similarities, cmap='coolwarm')\n",
    "    plt.xticks(range(len(words)), words, rotation=45)\n",
    "    plt.yticks(range(len(words)), words)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Word Similarity Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot similarities between key words\n",
    "key_words = ['frodo', 'sam', 'gandalf', 'white', 'mordor', 'shire', 'aragorn']\n",
    "plot_word_similarities(key_words, embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the differences between the two heatmaps\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Playing with Word Vectors [20 points]\n",
    "\n",
    "The intensive parts of this assignment are over - now we'll play with pretrained embeddings, i.e. embeddings that someone else has trained.\n",
    "\n",
    "We will use the GloVe embeddings from `gensim`, a Python library made for interacting with word vectors.\n",
    "\n",
    "In the cells below, we will make our imports, load in our embeddings, and construct our numpy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-18T19:01:15.702977Z",
     "iopub.status.idle": "2025-01-18T19:01:15.703435Z",
     "shell.execute_reply": "2025-01-18T19:01:15.703236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import gensim.downloader as api\n",
    "from pprint import pprint\n",
    "\n",
    "def load_embedding_model():\n",
    "    '''\n",
    "    Loads the GloVe embeddings from gensim\n",
    "    '''\n",
    "    gensim_wv = api.load(\"glove-wiki-gigaword-200\")\n",
    "    print(f\"Loaded embeddings with vocab size {len(gensim_wv.key_to_index)} with vector size {gensim_wv.vector_size}\")\n",
    "    return gensim_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-18T19:01:15.704646Z",
     "iopub.status.idle": "2025-01-18T19:01:15.705067Z",
     "shell.execute_reply": "2025-01-18T19:01:15.704929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load in the Embeddings (this can take ~8 minutes)\n",
    "gensim_wv = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-18T19:01:15.706211Z",
     "iopub.status.idle": "2025-01-18T19:01:15.706624Z",
     "shell.execute_reply": "2025-01-18T19:01:15.706444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_matrix(gensim_wv):\n",
    "    \n",
    "    # Get the words in the vocab\n",
    "    words = list(gensim_wv.index_to_key)\n",
    "    stoi = {}\n",
    "    M = []\n",
    "    curInd = 0\n",
    "    \n",
    "    # Add the words to the matrix M\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(gensim_wv.get_vector(w))\n",
    "            stoi[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # Convert the list of vectors to a numpy matrix\n",
    "    M = np.stack(M)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    return M, stoi\n",
    "\n",
    "M, stoi = get_embedding_matrix(gensim_wv)\n",
    "print(f\"Shape of the embedding matrix: {M.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our embeddings\n",
    "\n",
    "Now that we've created our matrix, let's work on visualizing them.\n",
    "\n",
    "The issue with these embeddings is that they are in 200 dimensions. Most humans can't see beyond 3 dimensions, and it's convenient to plot in 2.\n",
    "\n",
    "One nifty trick we can do to _squish_ down a vector in higher dimensions, to something in fewer dimensions, is to utilize **Dimensionality Reduction** techniques. This will learn the ambient structure in the data, and use it to capture as much information (technically, the \"variance\") in the amount of dimensions you want.\n",
    "\n",
    "Most people go with [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) or [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) for this. We will go with a variant of [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition), a method to factorize a matrix.\n",
    "\n",
    "You can read up on the documentation for the `sklearn` implementation [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html).\n",
    "\n",
    "In the cell below, implement the `reduce_to_k_dim` algorithm, where you run `TruncatedSVD` to squish your `(vocab_size, emb_dim)` matrix to `(vocab_size, K)`, where `K` is much smaller than `emb_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-18T19:01:15.707475Z",
     "iopub.status.idle": "2025-01-18T19:01:15.707840Z",
     "shell.execute_reply": "2025-01-18T19:01:15.707645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k=2):\n",
    "    '''\n",
    "    Reduce a matrix of shape (num_words, num_dim) to (num_words, k) dimensions\n",
    "    '''\n",
    "    n_iters = 10\n",
    "    print(f\"Running Truncated SVD over {n_iters} iterations...\")\n",
    "\n",
    "    ## Your code here\n",
    "        \n",
    "    ## --\n",
    "\n",
    "    return M_reduced\n",
    "\n",
    "# Reduce the matrix to 2 dimensions\n",
    "M_reduced = reduce_to_k_dim(M, k=2)\n",
    "\n",
    "# Normalize the rows to make them of unit length (helps with visualization)\n",
    "M_reduced_unit = M_reduced / np.linalg.norm(M_reduced, axis=1, keepdims=True)\n",
    "\n",
    "print(f\"Shape of the reduced matrix: {M_reduced_unit.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, you can plot out the embeddings from the reduced matrix. Note that since we squished the information coming from 200 dimensions into just 2, we won't have a perfect visualization by any means, but it's still worth studying.\n",
    "\n",
    "In the cell below, you can fill `words_to_plot` with words whose embeddings you'd like to see in a scatterplot. If you wish to join pairs of words, you can pass them in as pairs in the `pairs_to_join` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-18T19:01:15.708482Z",
     "iopub.status.idle": "2025-01-18T19:01:15.708887Z",
     "shell.execute_reply": "2025-01-18T19:01:15.708663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_embeddings(M_reduced_unit, words, stoi, pairs_to_join):\n",
    "    '''\n",
    "    Produces a scatterplot of the embeddings with the words annotated\n",
    "\n",
    "    Parameters:\n",
    "    M_reduced_unit : np.ndarray\n",
    "        The reduced matrix of embeddings\n",
    "    words : List[str]\n",
    "        The words to annotate\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    for i, txt in enumerate(words):\n",
    "        word_ind = stoi.get(txt)\n",
    "        ax.scatter(M_reduced_unit[word_ind, 0], M_reduced_unit[word_ind, 1])\n",
    "        ax.annotate(txt, (M_reduced_unit[word_ind, 0], M_reduced_unit[word_ind, 1]))\n",
    "\n",
    "    for pair in pairs_to_join:\n",
    "        w1, w2 = pair\n",
    "        w1_ind = stoi.get(w1)\n",
    "        w2_ind = stoi.get(w2)\n",
    "        ax.plot([M_reduced_unit[w1_ind, 0], M_reduced_unit[w2_ind, 0]], \n",
    "                [M_reduced_unit[w1_ind, 1], M_reduced_unit[w2_ind, 1]], 'k-')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "words_to_plot = [\"berlin\", \"germany\", \"paris\", \"france\", \"rome\", \"italy\", \"london\", \"england\"]\n",
    "pairs_to_join = [(\"berlin\", \"germany\"), (\"paris\", \"france\"), (\"rome\", \"italy\"), (\"london\", \"england\")]\n",
    "plot_embeddings(M_reduced_unit, words_to_plot, stoi, pairs_to_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies with Word Vectors\n",
    "\n",
    "Recall from the lectures that word vectors let us capture relationships between words. This means we can use vector arithmetic to create _analogies_.\n",
    "\n",
    "For example, if we had an embedding matrix E, and we wanted to find the relationship between `king` and `man`, and `queen` and `woman`, we would find\n",
    "\n",
    "$$E[\\text{king}] - E[\\text{man}] \\approx E[\\text{queen}] - E[\\text{woman}]$$\n",
    "\n",
    "`gensim` makes this really easy for us. To save time, we can use the `most_similar` function to capture the nearest neighbors to the vector you get when \"constructing the parallelogram\" (from lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-18T19:01:15.709958Z",
     "iopub.status.idle": "2025-01-18T19:01:15.710373Z",
     "shell.execute_reply": "2025-01-18T19:01:15.710190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# \"king is to man, as ??? is to woman\"\n",
    "gensim_wv.most_similar(\n",
    "    positive=['woman', 'king'],\n",
    "    negative=['man']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not perfect by any means. Run the cell below to see one case of the arithmetic failing.\n",
    "\n",
    "Write a few words about why this might be the case - there's a very reasonable explanation, provided you don't use the metric system ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-18T19:01:15.711365Z",
     "iopub.status.idle": "2025-01-18T19:01:15.711892Z",
     "shell.execute_reply": "2025-01-18T19:01:15.711619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# \"glove is to hand as ??? is to foot\"\n",
    "gensim_wv.most_similar(\n",
    "    positive=['foot', 'glove'],\n",
    "    negative=['hand']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green\"> Write your answer here. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, it's important to know that biases and stigmas are implicit inside these word embeddings. \n",
    "\n",
    "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"profession\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"profession\" and most dissimilar to \"woman\". \n",
    "\n",
    "Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-18T19:01:15.713156Z",
     "iopub.status.idle": "2025-01-18T19:01:15.713617Z",
     "shell.execute_reply": "2025-01-18T19:01:15.713439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
    "# most dissimilar from.\n",
    "pprint(gensim_wv.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
    "print('-'*25)\n",
    "pprint(gensim_wv.most_similar(positive=['woman', 'profession'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: green\"> Write your answer here. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6500906,
     "sourceId": 10499728,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
